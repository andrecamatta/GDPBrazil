{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bcb import sgs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import sidrapy as sidra\n",
    "from functools import reduce\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import yfinance as yf\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definindo o estilo do matplotlib para os gráficos\n",
    "mpl.style.use('bmh')\n",
    "\n",
    "def get_bcb_data():\n",
    "    # Dicionário com as séries desejadas\n",
    "    bcb_series = {\n",
    "        'BCB: PIB Mensal': 4380,\n",
    "        'BCB: Papel Moeda em Circulação': 1786,\n",
    "        'BCB: Reservas Bancárias': 1787,\n",
    "        'BCB: Papel Moeda em Poder do Público': 27789,\n",
    "        'BCB: M1': 27791,\n",
    "        'BCB: M2': 27810,\n",
    "        'BCB: M3': 27813,\n",
    "        'BCB: M4': 27815,\n",
    "        'BCB: Taxa de Câmbio USD Média Mensal': 3697,\n",
    "        'BCB: Índice de Commodities - Brasil - Agropecuária': 27575,\n",
    "        'BCB: Índice de Commodities - Brasil - Metal': 27576,\n",
    "        'BCB: Índice de Commodities - Brasil - Metal': 27577,\n",
    "        'BCB: SELIC acumulado no mês': 4390,\n",
    "        'FECOMERCIO: Índice de Confiança do Consumidor': 4393,\n",
    "        'FECOMERCIO: ìndice de Condições Econômicas Atuais': 4394,\n",
    "        'FGV: IGP-M': 189,\n",
    "        'ANP: Produção de Derivados do Petróleo - Total': 1391\n",
    "    }\n",
    "    # Baixando os dados do Banco Central usando o `bcb` para todas as séries\n",
    "    bcb_data = sgs.get(bcb_series, start='2003-01-01')\n",
    "    bcb_data.index = bcb_data.index.to_period('M')\n",
    "    return bcb_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sidra_series_adapt(df_sidra, column_name):\n",
    "    # Remover as linhas que não contêm dados relevantes (supondo que sejam as duas primeiras linhas)\n",
    "    df_sidra = df_sidra.iloc[2:].reset_index(drop=True)\n",
    "    \n",
    "    # Renomear as colunas abreviadas para nomes mais descritivos\n",
    "    df_sidra.rename(columns={\n",
    "        'D2C': 'Mês (Código)',\n",
    "        'V': 'Valor'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Selecionar apenas as colunas necessárias\n",
    "    df_sidra = df_sidra[['Mês (Código)', 'Valor']]\n",
    "    \n",
    "    # Converter 'Mês (Código)' em formato datetime\n",
    "    df_sidra['Date'] = pd.to_datetime(df_sidra['Mês (Código)'], format='%Y%m')\n",
    "\n",
    "    # Remover a coluna 'Mês (Código)', pois não é mais necessária\n",
    "    df_sidra.drop(columns=['Mês (Código)'], inplace=True)\n",
    "    \n",
    "    # Definir 'Date' como índice\n",
    "    df_sidra.set_index('Date', inplace=True)\n",
    "\n",
    "    df_sidra.index = df_sidra.index.to_period('M')\n",
    "    \n",
    "    # Converter a coluna 'Valor' para numérico\n",
    "    df_sidra['Valor'] = pd.to_numeric(df_sidra['Valor'], errors='coerce')\n",
    "\n",
    "    # Renomear a coluna 'Valor' para o nome desejado\n",
    "    df_sidra.rename(columns={'Valor': column_name}, inplace=True)\n",
    "\n",
    "    if isinstance(df_sidra.columns, pd.MultiIndex):\n",
    "        df_sidra.columns = [' '.join(col).strip() for col in df_sidra.columns.values]\n",
    "    \n",
    "    return df_sidra\n",
    "\n",
    "\n",
    "def get_sidra_series(column_name: str, table_code:str, variable:str, classifications:dict) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    column_name: str\n",
    "        Nome da coluna que será atribuída ao DataFrame final.\n",
    "    table_code: str\n",
    "        Código da tabela do SIDRA.\n",
    "    variable: str\n",
    "        Código da variável que será extraída.\n",
    "    classifications: str\n",
    "        Código da classificação que será extraída.\n",
    "    \"\"\"\n",
    "\n",
    "    if classifications:\n",
    "        df_sidra_brute = sidra.get_table(\n",
    "            table_code=table_code, \n",
    "            territorial_level=\"1\", \n",
    "            ibge_territorial_code=\"all\", \n",
    "            period=\"all\", \n",
    "            variable=variable, \n",
    "            classifications=classifications\n",
    "        )\n",
    "    else:\n",
    "        df_sidra_brute = sidra.get_table(\n",
    "            table_code=table_code, \n",
    "            territorial_level=\"1\", \n",
    "            ibge_territorial_code=\"all\", \n",
    "            period=\"all\", \n",
    "            variable=variable\n",
    "        )\n",
    "    # Remover as linhas que não contêm dados relevantes (supondo que sejam as duas primeiras linhas)\n",
    "    df_sidra = sidra_series_adapt(df_sidra_brute, column_name)\n",
    "\n",
    "    return df_sidra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidra_list = [\n",
    "    {\n",
    "        'column_name': 'IBGE: Indicador de Produção - Indústria de Transformação',\n",
    "        'table_code': '8888',\n",
    "        'variable': '12606',\n",
    "        'classifications': {'544': '129316'}\n",
    "    },\n",
    "    {\n",
    "        'column_name': 'IBGE: Indicador de Produção - Indústria Extrativa',\n",
    "        'table_code': '8888',\n",
    "        'variable': '12606',\n",
    "        'classifications': {'544': '129315'}\n",
    "    },\n",
    "    {\n",
    "        'column_name': 'IBGE: Indicador de Produção - Bens de Capital',\n",
    "        'table_code': '8887',\n",
    "        'variable': '12606',\n",
    "        'classifications': {'543': '129278'}\n",
    "    },\n",
    "    {\n",
    "        'column_name': 'IBGE: Indicador de Produção - Bens Intermediários',\n",
    "        'table_code': '8887',\n",
    "        'variable': '12606',\n",
    "        'classifications': {'543': '129283'}\n",
    "    },\n",
    "    {\n",
    "        'column_name': 'IBGE: Indicador de Produção - Bens de Consumo Duráveis',\n",
    "        'table_code': '8887',\n",
    "        'variable': '12606',\n",
    "        'classifications': {'543': '129301'}\n",
    "    },\n",
    "    {\n",
    "        'column_name': 'IBGE: Indicador de Produção - Bens de Consumo Semi-Duráveis',\n",
    "        'table_code': '8887',\n",
    "        'variable': '12606',\n",
    "        'classifications': {'543': '129306'}\n",
    "    },\n",
    "    {\n",
    "        'column_name': 'IBGE: Índice de Receita Nominal de Vendas no Comércio Varejista',\n",
    "        'table_code': '8880',\n",
    "        'variable': '7169',\n",
    "        'classifications': {'11046': '56733'}\n",
    "    },\n",
    "    {\n",
    "        'column_name': 'IBGE: Índice Nacional de Preços ao Consumidor Amplo (IPCA)',\n",
    "        'table_code': '1737',\n",
    "        'variable': '2266',\n",
    "        'classifications': None\n",
    "    },\n",
    "    {\n",
    "        'column_name': 'IBGE: Índice de Receita Nominal de Vendas de Material de Construção',\n",
    "        'table_code': '8757',\n",
    "        'variable': '7169',\n",
    "        'classifications': {'11046': '56731'}\n",
    "    }\n",
    "\n",
    "]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sidra_data(sidra_list: list) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    sidra_list: list\n",
    "        Lista de dicionários com as informações necessárias para baixar os dados do SIDRA.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicializar um DataFrame vazio\n",
    "    df_list = []\n",
    "    \n",
    "    for item in sidra_list:\n",
    "        column_name = item['column_name']\n",
    "        table_code = item['table_code']\n",
    "        variable = item['variable']\n",
    "        classifications = item['classifications']\n",
    "        \n",
    "        df_item = get_sidra_series(column_name, table_code, variable, classifications)\n",
    "        \n",
    "        df_list.append(df_item)\n",
    "\n",
    "        df_merged = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='outer'), df_list)\n",
    "\n",
    "    # Interessado somente nos dados a partir do ano de 2003\n",
    "    df_merged = df_merged[df_merged.index >= '2003-01-01']\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anfavea_production() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Faz o download do arquivo de séries temporais de produção de autoveículos da ANFAVEA,\n",
    "    processa os dados e retorna um DataFrame com as categorias de produção selecionadas.\n",
    "    \n",
    "    Retorno:\n",
    "        pd.DataFrame: DataFrame contendo as séries de produção para Automóveis,\n",
    "                      Comerciais Leves, Caminhões e Ônibus.\n",
    "    \"\"\"\n",
    "    # URL do arquivo Excel\n",
    "    url = \"https://anfavea.com.br/docs/SeriesTemporais_Autoveiculos.xlsm\"\n",
    "    \n",
    "    # Cabeçalhos para simular um navegador\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    # Fazer o download do arquivo\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Levanta erro se o download falhar\n",
    "    \n",
    "    # Abrir o arquivo Excel\n",
    "    file_content = BytesIO(response.content)\n",
    "    \n",
    "    # Combinar cabeçalhos (linha 4 e linha 5) para criar nomes descritivos\n",
    "    headers_df = pd.read_excel(file_content, sheet_name='Séries_Temporais_Autoveículos', nrows=5, header=None)\n",
    "    combined_headers = headers_df.iloc[3].ffill().astype(str) + ' - ' + headers_df.iloc[4].fillna('')\n",
    "    \n",
    "    # Ler os dados abaixo do cabeçalho combinado\n",
    "    df_with_headers = pd.read_excel(\n",
    "        file_content,\n",
    "        sheet_name='Séries_Temporais_Autoveículos',\n",
    "        header=5,  # Dados começam após as linhas de cabeçalho\n",
    "        names=combined_headers.fillna(''),  # Garantir que todos os nomes de coluna sejam strings\n",
    "    )\n",
    "    \n",
    "    # Identificar dinamicamente a coluna de datas\n",
    "    date_column = df_with_headers.columns[0]  # Geralmente é a primeira coluna\n",
    "    \n",
    "    # Selecionar as colunas relacionadas à produção\n",
    "    columns_of_interest = [col for col in df_with_headers.columns if 'Produção' in col]\n",
    "    \n",
    "    # Filtrar as colunas desejadas, incluindo a de data\n",
    "    df_producao = df_with_headers.loc[:, [date_column] + columns_of_interest].copy()\n",
    "    \n",
    "    # Renomear a coluna de data para 'Data'\n",
    "    df_producao.rename(columns={date_column: 'Data'}, inplace=True)\n",
    "    \n",
    "    # Ajustar os nomes das colunas para começar com \"Anfavea:\" e formatar adequadamente\n",
    "    column_mapping = {col: f\"Anfavea: Produção de {col.split(' - ')[0].capitalize()}\" for col in columns_of_interest}\n",
    "    df_producao.rename(columns=column_mapping, inplace=True)\n",
    "    \n",
    "    # Converter a coluna 'Data' para datetime e organizar como índice mensal\n",
    "    df_producao['Data'] = pd.to_datetime(df_producao['Data'], errors='coerce')\n",
    "    df_producao = df_producao.dropna(subset=['Data'])  # Remover linhas com datas inválidas\n",
    "    df_producao.set_index(df_producao['Data'].dt.to_period('M'), inplace=True)\n",
    "    df_producao.drop(columns=['Data'], inplace=True)\n",
    "    \n",
    "    # Converter colunas de produção para valores numéricos\n",
    "    df_producao = df_producao.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    df_producao = df_producao[df_producao.index >= '2003-01-01']\n",
    "    \n",
    "    return df_producao\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Baixa um arquivo a partir da URL fornecida.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    with open(output_path, \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)\n",
    "    print(f\"Arquivo baixado: {output_path}\")\n",
    "\n",
    "def epe_mwh_consumption(file_path: str, sheet_name: str, row_identifier: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrai dados mensais de consumo para uma linha específica (e.g., \"TOTAL RESIDENCIAL\") para todos os anos disponíveis.\n",
    "    \"\"\"\n",
    "    # Carregar a aba\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name, header=None, engine=\"xlrd\")\n",
    "    \n",
    "    # Encontrar as células com todas as ocorrências do identificador de linha\n",
    "    row_indices = df[df[0] == row_identifier].index\n",
    "    # Adicionar também aqueles com \"TOTAL BRASIL\"\n",
    "    row_indices = row_indices.union(df[df[0] == \"TOTAL BRASIL\"].index)\n",
    "\n",
    "    # Checa se o elemento da segunda coluna dos índices é \"JAN\", se for, acrescenta 1 ao índice (célula foi mesclada)\n",
    "    row_indices = [row_index + 1 if df.iloc[row_index, 1] == \"JAN\" else row_index for row_index in row_indices]\n",
    "    # Para cada uma dessas ocorrências, extrair o ano correspondente, o ano está na segunda coluna, duas linhas acima, e é um número de quatro dígitos\n",
    "    # ou um número de quatro dígitos seguido de asterisco, associar portanto, a linha de ocorrencia do identificador de linha com o ano correspondente\n",
    "    row_years = {row_index: df.iloc[row_index - 2, 1] for row_index in row_indices}\n",
    "    # Para as ocorrências em que houver o asterisco, remover o asterisco, e transformar o valor em inteiro\n",
    "    row_years = {row_index: int(str(year).replace(\"*\", \"\")) for row_index, year in row_years.items()}\n",
    "    # Os valores que estivem na linha de ocorrencia do identificador de linha, são os valores de consumo mensal e deve ser lido até o 12º mês\n",
    "    row_values = {row_index: df.iloc[row_index, 1:13].values for row_index in row_indices}\n",
    "    # Criar dataframe em que o índice é um period M (ano-mês) e a coluna é o valor de consumo mensal, na ordem 01, 02, ..., 12\n",
    "    # Se o valor de consumo mensal for zero, a linha não é incluída\n",
    "    data = {\n",
    "        pd.Period(year=year, month=month, freq=\"M\"): value\n",
    "        for row_index, year in row_years.items()\n",
    "        for month, value in enumerate(row_values[row_index], start=1)\n",
    "        if value != 0\n",
    "    }\n",
    "    # O padrão do nome da coluna é EPE: Consumo de Energia Elétrica (Residencial, Industrial, Comercial) em MWh\n",
    "    col_name = f\"EPE: Consumo de Energia Elétrica ({sheet_name.capitalize()}) em MWh\" \n",
    "\n",
    "    # Converter para dataframe\n",
    "    df = pd.DataFrame(data, index=[\"Consumo\"]).T\n",
    "    df.columns = [col_name]\n",
    "    # Ordena o índice do mais antigo para o mais novo\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_epe_data():\n",
    "    # URL e caminho para download e extração dos dados\n",
    "    url = \"https://www.epe.gov.br/sites-pt/publicacoes-dados-abertos/publicacoes/Documents/CONSUMO%20MENSAL%20DE%20ENERGIA%20EL%C3%89TRICA%20POR%20CLASSE.xls\"\n",
    "    xls_path = \"CONSUMO_MENSAL_ENERGIA.xls\"\n",
    "\n",
    "    # Baixar o arquivo\n",
    "    download_file(url, xls_path)\n",
    "\n",
    "    # Extrair dados para cada setor (Residencial, Industrial, Comercial)\n",
    "    sectors = {\n",
    "        \"Residencial\": {\"sheet_name\": \"RESIDENCIAL\", \"row_identifier\": \"TOTAL RESIDENCIAL\"},\n",
    "        \"Industrial\": {\"sheet_name\": \"INDUSTRIAL\", \"row_identifier\": \"TOTAL INDUSTRIAL (CATIVO + LIVRE)\"},\n",
    "        \"Comercial\": {\"sheet_name\": \"COMERCIAL\", \"row_identifier\": \"TOTAL COMERCIAL (CATIVO + LIVRE)\"},\n",
    "    }\n",
    "\n",
    "    # Extrai  para os setores e faz um merge em um dataframe único\n",
    "    df_list = []\n",
    "    for sector, params in sectors.items():\n",
    "        df_sector = epe_mwh_consumption(xls_path, params[\"sheet_name\"], params[\"row_identifier\"])\n",
    "        df_list.append(df_sector)\n",
    "\n",
    "    df_merged = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='outer'), df_list)\n",
    "    # Apaga o arquivo\n",
    "    os.remove(xls_path)\n",
    "    return df_merged\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_market_data():\n",
    "    # Lista dos tickers selecionados\n",
    "    tickers = {\n",
    "        \"^BVSP\": \"Mkt: Ibovespa\",           # Ibovespa\n",
    "        \"CL=F\": \"Mkt: Petróleo (WTI)\",      # Petróleo WTI\n",
    "        \"ZS=F\": \"Mkt: Soja\",                # Soja\n",
    "        \"KC=F\": \"Mkt: Café\",                # Café\n",
    "        \"ZC=F\": \"Mkt: Milho\",               # Milho\n",
    "        \"SB=F\": \"Mkt: Açúcar\",              # Açúcar\n",
    "        \"GC=F\": \"Mkt: Ouro\",                # Ouro\n",
    "        \"^SPX\": \"Mkt: S&P 500\",             # S&P 500\n",
    "        \"EEM\": \"Mkt: Mercados Emergentes\",  # ETF de Mercados Emergentes\n",
    "        \"^DJT\": \"Mkt: Transportes EUA\",     # Índice de Transportes Dow Jones\n",
    "     }\n",
    "\n",
    "    # Inicializa um DataFrame vazio para consolidar os dados\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    # Baixa os dados de cada ticker e adiciona ao DataFrame consolidado\n",
    "    for ticker, label in tickers.items():\n",
    "        print(f\"Baixando dados para {label} {ticker}...\")\n",
    "        data = yf.download(ticker, start=\"2000-01-01\", end=\"2025-12-31\", interval=\"1mo\")\n",
    "        \n",
    "        # Apenas a coluna de fechamento ajustado, renomeada para o indicador\n",
    "        data = data[['Adj Close']].rename(columns={'Adj Close': label})\n",
    "        \n",
    "        # Ajusta o índice para períodos mensais\n",
    "        data.index = data.index.to_period('M')\n",
    "        \n",
    "        # Combina com o DataFrame principal\n",
    "        if all_data.empty:\n",
    "            all_data = data\n",
    "        else:\n",
    "            all_data = all_data.join(data, how=\"outer\")\n",
    "\n",
    "    # Visualiza as primeiras linhas do DataFrame consolidado\n",
    "    all_data = all_data[all_data.index >= '2003-01-01']\n",
    "\n",
    "    # Faz Impute dos valores faltantes, interpolando linearmente\n",
    "    all_data = all_data.interpolate(method='linear')\n",
    "\n",
    "    print(all_data.head())\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_process_rpk_data():\n",
    "    base_url = \"https://www.gov.br/anac/pt-br/assuntos/dados-e-estatisticas/dados-estatisticos/arquivos/resumo_anual_{}.csv\"\n",
    "    start_year = 2003\n",
    "    end_year = 2024  # Atualize conforme necessário\n",
    "    combined_data = []\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        url = base_url.format(year)\n",
    "        local_file = f\"resumo_anual_{year}.csv\"\n",
    "        \n",
    "        # Download do arquivo\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(local_file, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Baixado: {local_file}\")\n",
    "\n",
    "            # Carregar os dados\n",
    "            try:\n",
    "                data = pd.read_csv(local_file, encoding='latin1', sep=';', low_memory=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar o arquivo {local_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Filtrar voos domésticos\n",
    "            domestic_data = data.loc[\n",
    "                (data['AEROPORTO DE ORIGEM (PAÍS)'] == 'BRASIL') &\n",
    "                (data['AEROPORTO DE DESTINO (PAÍS)'] == 'BRASIL')\n",
    "            ].copy()\n",
    "\n",
    "            # Converter RPK para numérico\n",
    "            domestic_data['RPK'] = pd.to_numeric(domestic_data['CARGA PAGA KM'], errors='coerce')\n",
    "\n",
    "            # Agrupar por ano e mês e somar o RPK\n",
    "            monthly_rpk = domestic_data.groupby(['ANO', 'MÊS'])['RPK'].sum().reset_index()\n",
    "            combined_data.append(monthly_rpk)\n",
    "\n",
    "            # Remover o arquivo baixado\n",
    "            os.remove(local_file)\n",
    "            print(f\"Arquivo removido: {local_file}\")\n",
    "        else:\n",
    "            print(f\"Erro ao baixar o arquivo para o ano {year}: {response.status_code}\")\n",
    "\n",
    "    # Combinar todos os anos em um único DataFrame\n",
    "    if combined_data:\n",
    "        final_data = pd.concat(combined_data, ignore_index=True)\n",
    "        # Criar uma coluna de período\n",
    "        final_data['PERIOD'] = pd.to_datetime(final_data['ANO'].astype(str) + '-' + final_data['MÊS'].astype(str) + '-01')\n",
    "        final_data['PERIOD'] = final_data['PERIOD'].dt.to_period('M')\n",
    "        final_data = final_data[['PERIOD', 'RPK']].set_index('PERIOD')\n",
    "        # Ordenar os dados\n",
    "        final_data = final_data.sort_index()\n",
    "        # O nome da coluna deve ser ANAC: RPK (Passageiro x Quilômetro)\n",
    "        final_data.columns = ['ANAC: RPK (Passageiro x Quilômetro)']\n",
    "        return final_data\n",
    "    else:\n",
    "        print(\"Nenhum dado foi processado.\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limites geográficos das regiões\n",
    "regions = {\n",
    "    \"Norte\": {\"lat_min\": 0, \"lat_max\": 5, \"lon_min\": -73, \"lon_max\": -45},\n",
    "    \"Nordeste\": {\"lat_min\": -17, \"lat_max\": 0, \"lon_min\": -47, \"lon_max\": -35},\n",
    "    \"Centro-Oeste\": {\"lat_min\": -24, \"lat_max\": -7, \"lon_min\": -61, \"lon_max\": -45},\n",
    "    \"Sudeste\": {\"lat_min\": -24, \"lat_max\": -17, \"lon_min\": -48, \"lon_max\": -40},\n",
    "    \"Sul\": {\"lat_min\": -34, \"lat_max\": -24, \"lon_min\": -57, \"lon_max\": -48},\n",
    "}\n",
    "\n",
    "def aggregate_spatially(df):\n",
    "    \"\"\"\n",
    "    Agrega espacialmente os dados de todos os pontos da região.\n",
    "    Calcula as médias diárias de cada variável.\n",
    "    \"\"\"\n",
    "\n",
    "    # Média diária para todos os pontos\n",
    "    daily_spatial_avg = df.groupby(\"date\").mean().reset_index()\n",
    "    \n",
    "    # Remover as colunas de latitude e longitude, pois não são mais necessárias\n",
    "    daily_spatial_avg = daily_spatial_avg.drop(columns=[\"latitude\", \"longitude\"], errors=\"ignore\")\n",
    "    return daily_spatial_avg\n",
    "\n",
    "\n",
    "def aggregate_monthly_data(df):\n",
    "    \"\"\"\n",
    "    Agrega os dados diários em valores mensais:\n",
    "    - Média para T2M\n",
    "    - Mínimo para T2M_MIN\n",
    "    - Máximo para T2M_MAX\n",
    "    - Soma para PRECTOTCORR\n",
    "    \"\"\"\n",
    "    # Converte para um índice de data para facilitar a manipulação\n",
    "    df[\"month\"] = df[\"date\"].dt.to_period(\"M\")\n",
    "\n",
    "    # Agregação por mês\n",
    "    monthly_data = df.groupby(\"month\").agg({\n",
    "        \"T2M\": \"mean\",              # Média da temperatura média\n",
    "        \"T2M_MIN\": \"min\",           # Mínima da temperatura mínima\n",
    "        \"T2M_MAX\": \"max\",           # Máxima da temperatura máxima\n",
    "        \"PRECTOTCORR\": \"sum\",       # Soma da precipitação\n",
    "    }).reset_index()\n",
    "\n",
    "    # o month passará a ser o índice\n",
    "    monthly_data.set_index(\"month\", inplace=True)\n",
    "    return monthly_data\n",
    "\n",
    "\n",
    "def get_nasa_power_data(latitude, longitude, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Coleta dados históricos da NASA POWER API para um ponto específico.\n",
    "    \"\"\"\n",
    "    url = f\"https://power.larc.nasa.gov/api/temporal/daily/point\"\n",
    "    params = {\n",
    "        \"parameters\": \"T2M,T2M_MIN,T2M_MAX,PRECTOTCORR\",  # Temperatura e precipitação\n",
    "        \"community\": \"AG\",\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start\": start_date,\n",
    "        \"end\": end_date,\n",
    "        \"format\": \"JSON\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()[\"properties\"][\"parameter\"]\n",
    "        df = pd.DataFrame(data)\n",
    "        # Criar a coluna de data a partir do índice\n",
    "        df[\"date\"] = df.index\n",
    "        # O índice é da forma '20030101', converter para date \n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y%m%d\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Erro na API para ({latitude}, {longitude}):\", response.status_code)\n",
    "        return None\n",
    "\n",
    "def process_region_with_aggregation(region_name, limits, start_date, end_date, step=4):\n",
    "    \"\"\"\n",
    "    Processa os dados diários de uma região e os agrega em médias mensais.\n",
    "    \"\"\"\n",
    "    lat_min, lat_max = limits[\"lat_min\"], limits[\"lat_max\"]\n",
    "    lon_min, lon_max = limits[\"lon_min\"], limits[\"lon_max\"]\n",
    "\n",
    "    latitudes = np.arange(lat_min, lat_max, step)\n",
    "    longitudes = np.arange(lon_min, lon_max, step)\n",
    "\n",
    "    data_frames = []\n",
    "\n",
    "    # Coletar dados diários para cada ponto na grade\n",
    "    for lat in latitudes:\n",
    "        for lon in longitudes:\n",
    "            df = get_nasa_power_data(lat, lon, start_date, end_date)\n",
    "            if df is not None:\n",
    "                df[\"latitude\"] = lat\n",
    "                df[\"longitude\"] = lon\n",
    "                data_frames.append(df)\n",
    "\n",
    "    # Combinar dados de todos os pontos\n",
    "    combined_df = pd.concat(data_frames)\n",
    "\n",
    "    # Antes da consolidação, elimina as linhas em que pelo menos um dos valores é menor que -100 (valor inválido)\n",
    "    # Imprime a data e a localização do ponto com valores inválidos\n",
    "    invalid_data = combined_df[(combined_df[\"T2M\"] < -100) | (combined_df[\"T2M_MIN\"] < -100) | (combined_df[\"T2M_MAX\"] < -100) | (combined_df[\"PRECTOTCORR\"] < -100)]\n",
    "    if not invalid_data.empty:\n",
    "        print(f\"Valores inválidos encontrados para {region_name}:\")\n",
    "        print(invalid_data[[\"date\", \"latitude\", \"longitude\"]])\n",
    "\n",
    "    combined_df = combined_df[(combined_df[\"T2M\"] >= -100) & (combined_df[\"T2M_MIN\"] >= -100) & (combined_df[\"T2M_MAX\"] >= -100) & (combined_df[\"PRECTOTCORR\"] >= -100)]\n",
    "\n",
    "    spatially_aggregated = aggregate_spatially(combined_df)\n",
    "\n",
    "    # Agregar os dados em médias mensais\n",
    "    monthly_data = aggregate_monthly_data(spatially_aggregated)\n",
    "\n",
    "    print(f\"Dados agregados mensalmente para {region_name}\")\n",
    "    return monthly_data\n",
    "\n",
    "\n",
    "def consolidate_regions(region_data):\n",
    "    \"\"\"\n",
    "    Cria um DataFrame consolidado com colunas formatadas para cada variável e região.\n",
    "    \"\"\"\n",
    "    consolidated_data = pd.DataFrame()\n",
    "\n",
    "    for region_name, data in region_data.items():\n",
    "        # Renomear colunas para incluir o nome da região\n",
    "        data = data.rename(columns={\n",
    "            \"T2M_MIN\": f\"NASA Power: Temp. Min.(C)/{region_name}\",\n",
    "            \"T2M_MAX\": f\"NASA Power: Temp. Máx.(C)/{region_name}\",\n",
    "            \"PRECTOTCORR\": f\"NASA Power: Precipitação(mm)/{region_name}\"\n",
    "        })\n",
    "\n",
    "        # Selecionar apenas as colunas necessárias\n",
    "        data = data[[f\"NASA Power: Temp. Min.(C)/{region_name}\", f\"NASA Power: Temp. Máx.(C)/{region_name}\", f\"NASA Power: Precipitação(mm)/{region_name}\"]]\n",
    "\n",
    "        # Concatenar com o DataFrame consolidado\n",
    "        if consolidated_data.empty:\n",
    "            consolidated_data = data\n",
    "        else:\n",
    "            consolidated_data = pd.concat([consolidated_data, data], axis=1)\n",
    "\n",
    "    return consolidated_data\n",
    "\n",
    "\n",
    "def get_climate_data():\n",
    "    start_date = \"20030101\"\n",
    "    end_date = \"20241031\"\n",
    "    region_data = {}\n",
    "\n",
    "    for region_name, limits in regions.items():\n",
    "        print(f\"Processando dados para a região: {region_name}\")\n",
    "        region_data[region_name] = process_region_with_aggregation(region_name, limits, start_date, end_date)\n",
    "\n",
    "    consolidated_df = consolidate_regions(region_data)\n",
    "    return consolidated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comex_data():\n",
    "    # URLs dos arquivos\n",
    "    urls = {\n",
    "        \"exportacao\": \"https://balanca.economia.gov.br/balanca/bd/comexstat-bd/ncm/EXP_COMPLETA.zip\",\n",
    "        \"importacao\": \"https://balanca.economia.gov.br/balanca/bd/comexstat-bd/ncm/IMP_COMPLETA.zip\"\n",
    "    }\n",
    "\n",
    "    # Função para baixar, descompactar e processar os dados\n",
    "    def process_comex_data(url, operation_type):\n",
    "        print(f\"Baixando dados de {operation_type}...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            # Salvar o arquivo ZIP localmente\n",
    "            zip_path = f\"temp_{operation_type}.zip\"\n",
    "            with open(zip_path, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "            # Extrair o conteúdo do arquivo ZIP\n",
    "            extract_dir = f\"extracted_{operation_type}\"\n",
    "            os.makedirs(extract_dir, exist_ok=True)\n",
    "            shutil.unpack_archive(zip_path, extract_dir, \"zip\")\n",
    "\n",
    "            # Localizar o arquivo CSV extraído\n",
    "            csv_file = [f for f in os.listdir(extract_dir) if f.endswith(\".csv\")][0]\n",
    "            csv_path = os.path.join(extract_dir, csv_file)\n",
    "\n",
    "            # Ler o CSV e processar os dados\n",
    "            print(f\"Lendo o arquivo {csv_file} para {operation_type}...\")\n",
    "            df = pd.read_csv(csv_path, sep=\";\", usecols=[\"CO_ANO\", \"CO_MES\", \"VL_FOB\"])\n",
    "\n",
    "            # Criar a coluna de período e agregar os valores\n",
    "            df[\"month\"] = pd.to_datetime(df[\"CO_ANO\"].astype(str) + df[\"CO_MES\"].astype(str), format=\"%Y%m\")\n",
    "\n",
    "            df = df.groupby(\"month\")[\"VL_FOB\"].sum().sort_index().to_frame()\n",
    "\n",
    "            # Renomear a coluna para identificar o tipo de operação\n",
    "            df.columns = [f\"VL_FOB_{operation_type}\"]\n",
    "\n",
    "            # Limpar arquivos temporários\n",
    "            shutil.rmtree(extract_dir)\n",
    "            os.remove(zip_path)\n",
    "\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"Erro ao baixar {operation_type}: {response.status_code}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    # Processar exportações e importações\n",
    "    export_data = process_comex_data(urls[\"exportacao\"], \"exportacao\")\n",
    "    import_data = process_comex_data(urls[\"importacao\"], \"importacao\")\n",
    "\n",
    "    # Combinar os dados\n",
    "    combined_data = pd.concat([export_data, import_data], axis=1).fillna(0)\n",
    "\n",
    "    # O month já é o índice dos DataFrames export_data e import_data\n",
    "    combined_data.index.name = \"month\"\n",
    "\n",
    "    # Converter para period M\n",
    "    combined_data.index = combined_data.index.to_period(\"M\")\n",
    "            \n",
    "    # Ordenar pelo índice e salvar o arquivo consolidado\n",
    "    combined_data = combined_data.sort_index()\n",
    "\n",
    "    # Filtra para partir de 2003\n",
    "    combined_data = combined_data[combined_data.index >= '2003-01-01']\n",
    "    # A coluna de importação deve se chamar MDIC: Valor de Importação FOB e de exportação MDIC: Valor de Exportação FOB\n",
    "    combined_data.columns = [\"MDIC: Valor de Exportação (FOB)\", \"MDIC: Valor de Importação (FOB)\"]\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unemployment_data():\n",
    "    # A URL do PME é da forma https://apisidra.ibge.gov.br/values/t/2179/n110/all/v/1035/p/last%201/d/v1035%201\n",
    "    # Baixar com sidra.get_table\n",
    "    df_pme_brute = sidra.get_table(\n",
    "        table_code=\"2179\",\n",
    "        territorial_level=\"110\",\n",
    "        ibge_territorial_code=\"all\",\n",
    "        variable=\"1035\",\n",
    "        period=\"all\",\n",
    "        header=0\n",
    "    )\n",
    "\n",
    "    df_pme = sidra_series_adapt(df_pme_brute, \"IBGE: Taxa de Desocupação\")\n",
    "\n",
    "    df_pnad = sidra.get_table(\n",
    "        table_code=\"6381\",\n",
    "        territorial_level=\"1\",\n",
    "        ibge_territorial_code=\"all\",\n",
    "        variable=\"4099\",\n",
    "        period=\"all\",\n",
    "        header=0\n",
    "    )\n",
    "\n",
    "    df_pnad = sidra_series_adapt(df_pnad, \"IBGE: Taxa de Desocupação\")\n",
    "    \n",
    "    # Combinar, verticalmente, os dados do PME e PNAD, se a data for a mesma, preferir PNAD\n",
    "    # Ou seja, um dataframe com data e taxa de desocupação é o resultado\n",
    "    combined_unemployment_rate = pd.concat([df_pme, df_pnad]).sort_index()\n",
    "    combined_unemployment_rate = combined_unemployment_rate[~combined_unemployment_rate.index.duplicated(keep='last')]\n",
    "\n",
    "    # Filtrar para partir de 2003\n",
    "    combined_unemployment_rate = combined_unemployment_rate[combined_unemployment_rate.index >= '2003-01-01']\n",
    "    return combined_unemployment_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando dados para a região: Norte\n",
      "Baixando dados de exportacao...\n",
      "Baixando dados para Mkt: Ibovespa ^BVSP...\n",
      "Arquivo baixado: CONSUMO_MENSAL_ENERGIA.xls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipykernel_27638/3381059830.py:28: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  data.index = data.index.to_period('M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando dados para Mkt: Petróleo (WTI) CL=F...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipykernel_27638/3381059830.py:28: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  data.index = data.index.to_period('M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando dados para Mkt: Soja ZS=F...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipykernel_27638/3381059830.py:28: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  data.index = data.index.to_period('M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando dados para Mkt: Café KC=F...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipykernel_27638/3381059830.py:28: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  data.index = data.index.to_period('M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando dados para Mkt: Milho ZC=F...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipykernel_27638/3381059830.py:28: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  data.index = data.index.to_period('M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando dados para Mkt: Açúcar SB=F...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipykernel_27638/3381059830.py:28: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  data.index = data.index.to_period('M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando dados para Mkt: Ouro GC=F...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipykernel_27638/3381059830.py:28: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  data.index = data.index.to_period('M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando dados para Mkt: S&P 500 ^SPX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipykernel_27638/3381059830.py:28: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  data.index = data.index.to_period('M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando dados para Mkt: Mercados Emergentes EEM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipykernel_27638/3381059830.py:28: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  data.index = data.index.to_period('M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando dados para Mkt: Transportes EUA ^DJT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/tmp/ipykernel_27638/3381059830.py:28: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  data.index = data.index.to_period('M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price   Mkt: Ibovespa Mkt: Petróleo (WTI) Mkt: Soja  Mkt: Café Mkt: Milho  \\\n",
      "Ticker          ^BVSP                CL=F      ZS=F       KC=F       ZC=F   \n",
      "Date                                                                        \n",
      "2003-01       10941.0           33.509998     564.0  65.300003     238.25   \n",
      "2003-02       10281.0           36.599998     577.0  56.849998     231.75   \n",
      "2003-03       11274.0           31.040001     574.5  58.650002     236.50   \n",
      "2003-04       12557.0           25.799999     623.5  68.250000     232.75   \n",
      "2003-05       13422.0           29.559999     624.5  58.349998     244.25   \n",
      "\n",
      "Price   Mkt: Açúcar   Mkt: Ouro Mkt: S&P 500 Mkt: Mercados Emergentes  \\\n",
      "Ticker         SB=F        GC=F         ^SPX                      EEM   \n",
      "Date                                                                    \n",
      "2003-01        8.64  368.299988   855.700012                      NaN   \n",
      "2003-02        8.90  350.200012   841.150024                      NaN   \n",
      "2003-03        7.68  335.899994   848.179993                      NaN   \n",
      "2003-04        7.30  339.100006   916.919983                      NaN   \n",
      "2003-05        7.08  364.500000   963.590027                 8.586379   \n",
      "\n",
      "Price   Mkt: Transportes EUA  \n",
      "Ticker                  ^DJT  \n",
      "Date                          \n",
      "2003-01          2173.350098  \n",
      "2003-02          2049.050049  \n",
      "2003-03          2131.209961  \n",
      "2003-04          2408.870117  \n",
      "2003-05          2486.350098  \n",
      "Baixado: resumo_anual_2003.csv\n",
      "Arquivo removido: resumo_anual_2003.csv\n",
      "Baixado: resumo_anual_2004.csv\n",
      "Arquivo removido: resumo_anual_2004.csv\n",
      "Baixado: resumo_anual_2005.csv\n",
      "Arquivo removido: resumo_anual_2005.csv\n",
      "Baixado: resumo_anual_2006.csv\n",
      "Arquivo removido: resumo_anual_2006.csv\n",
      "Baixado: resumo_anual_2007.csv\n",
      "Lendo o arquivo EXP_COMPLETA.csv para exportacao...\n",
      "Arquivo removido: resumo_anual_2007.csv\n",
      "Baixando dados de importacao...\n",
      "Baixado: resumo_anual_2008.csv\n",
      "Arquivo removido: resumo_anual_2008.csv\n",
      "Baixado: resumo_anual_2009.csv\n",
      "Arquivo removido: resumo_anual_2009.csv\n",
      "Valores inválidos encontrados para Norte:\n",
      "               date  latitude  longitude\n",
      "20240712 2024-07-12         0        -73\n",
      "20240713 2024-07-13         0        -73\n",
      "20240714 2024-07-14         0        -73\n",
      "20240712 2024-07-12         0        -69\n",
      "20240713 2024-07-13         0        -69\n",
      "20240714 2024-07-14         0        -69\n",
      "20240712 2024-07-12         0        -65\n",
      "20240713 2024-07-13         0        -65\n",
      "20240714 2024-07-14         0        -65\n",
      "20240712 2024-07-12         0        -61\n",
      "20240713 2024-07-13         0        -61\n",
      "20240714 2024-07-14         0        -61\n",
      "20240712 2024-07-12         0        -57\n",
      "20240713 2024-07-13         0        -57\n",
      "20240714 2024-07-14         0        -57\n",
      "20240712 2024-07-12         0        -53\n",
      "20240713 2024-07-13         0        -53\n",
      "20240714 2024-07-14         0        -53\n",
      "20240712 2024-07-12         0        -49\n",
      "20240713 2024-07-13         0        -49\n",
      "20240714 2024-07-14         0        -49\n",
      "20240712 2024-07-12         4        -73\n",
      "20240713 2024-07-13         4        -73\n",
      "20240714 2024-07-14         4        -73\n",
      "20240712 2024-07-12         4        -69\n",
      "20240713 2024-07-13         4        -69\n",
      "20240714 2024-07-14         4        -69\n",
      "20240712 2024-07-12         4        -65\n",
      "20240713 2024-07-13         4        -65\n",
      "20240714 2024-07-14         4        -65\n",
      "20240712 2024-07-12         4        -61\n",
      "20240713 2024-07-13         4        -61\n",
      "20240714 2024-07-14         4        -61\n",
      "20240712 2024-07-12         4        -57\n",
      "20240713 2024-07-13         4        -57\n",
      "20240714 2024-07-14         4        -57\n",
      "20240712 2024-07-12         4        -53\n",
      "20240713 2024-07-13         4        -53\n",
      "20240714 2024-07-14         4        -53\n",
      "20240712 2024-07-12         4        -49\n",
      "20240713 2024-07-13         4        -49\n",
      "20240714 2024-07-14         4        -49\n",
      "Dados agregados mensalmente para Norte\n",
      "Processando dados para a região: Nordeste\n",
      "Baixado: resumo_anual_2010.csv\n",
      "Arquivo removido: resumo_anual_2010.csv\n",
      "Baixado: resumo_anual_2011.csv\n",
      "Arquivo removido: resumo_anual_2011.csv\n",
      "Baixado: resumo_anual_2012.csv\n",
      "Arquivo removido: resumo_anual_2012.csv\n",
      "Baixado: resumo_anual_2013.csv\n",
      "Arquivo removido: resumo_anual_2013.csv\n",
      "Baixado: resumo_anual_2014.csv\n",
      "Arquivo removido: resumo_anual_2014.csv\n",
      "Baixado: resumo_anual_2015.csv\n",
      "Arquivo removido: resumo_anual_2015.csv\n",
      "Baixado: resumo_anual_2016.csv\n",
      "Arquivo removido: resumo_anual_2016.csv\n",
      "Baixado: resumo_anual_2017.csv\n",
      "Arquivo removido: resumo_anual_2017.csv\n",
      "Baixado: resumo_anual_2018.csv\n",
      "Arquivo removido: resumo_anual_2018.csv\n",
      "Lendo o arquivo IMP_COMPLETA.csv para importacao...\n",
      "Baixado: resumo_anual_2019.csv\n",
      "Arquivo removido: resumo_anual_2019.csv\n",
      "Valores inválidos encontrados para Nordeste:\n",
      "               date  latitude  longitude\n",
      "20240712 2024-07-12       -17        -47\n",
      "20240713 2024-07-13       -17        -47\n",
      "20240714 2024-07-14       -17        -47\n",
      "20240712 2024-07-12       -17        -43\n",
      "20240713 2024-07-13       -17        -43\n",
      "20240714 2024-07-14       -17        -43\n",
      "20240712 2024-07-12       -17        -39\n",
      "20240713 2024-07-13       -17        -39\n",
      "20240714 2024-07-14       -17        -39\n",
      "20240712 2024-07-12       -13        -47\n",
      "20240713 2024-07-13       -13        -47\n",
      "20240714 2024-07-14       -13        -47\n",
      "20240712 2024-07-12       -13        -43\n",
      "20240713 2024-07-13       -13        -43\n",
      "20240714 2024-07-14       -13        -43\n",
      "20240712 2024-07-12       -13        -39\n",
      "20240713 2024-07-13       -13        -39\n",
      "20240714 2024-07-14       -13        -39\n",
      "20240712 2024-07-12        -9        -47\n",
      "20240713 2024-07-13        -9        -47\n",
      "20240714 2024-07-14        -9        -47\n",
      "20240712 2024-07-12        -9        -43\n",
      "20240713 2024-07-13        -9        -43\n",
      "20240714 2024-07-14        -9        -43\n",
      "20240712 2024-07-12        -9        -39\n",
      "20240713 2024-07-13        -9        -39\n",
      "20240714 2024-07-14        -9        -39\n",
      "20240712 2024-07-12        -5        -47\n",
      "20240713 2024-07-13        -5        -47\n",
      "20240714 2024-07-14        -5        -47\n",
      "20240712 2024-07-12        -5        -43\n",
      "20240713 2024-07-13        -5        -43\n",
      "20240714 2024-07-14        -5        -43\n",
      "20240712 2024-07-12        -5        -39\n",
      "20240713 2024-07-13        -5        -39\n",
      "20240714 2024-07-14        -5        -39\n",
      "20240712 2024-07-12        -1        -47\n",
      "20240713 2024-07-13        -1        -47\n",
      "20240714 2024-07-14        -1        -47\n",
      "20240712 2024-07-12        -1        -43\n",
      "20240713 2024-07-13        -1        -43\n",
      "20240714 2024-07-14        -1        -43\n",
      "20240712 2024-07-12        -1        -39\n",
      "20240713 2024-07-13        -1        -39\n",
      "20240714 2024-07-14        -1        -39\n",
      "Dados agregados mensalmente para Nordeste\n",
      "Processando dados para a região: Centro-Oeste\n",
      "Baixado: resumo_anual_2020.csv\n",
      "Arquivo removido: resumo_anual_2020.csv\n",
      "Baixado: resumo_anual_2021.csv\n",
      "Arquivo removido: resumo_anual_2021.csv\n",
      "Baixado: resumo_anual_2022.csv\n",
      "Arquivo removido: resumo_anual_2022.csv\n",
      "Baixado: resumo_anual_2023.csv\n",
      "Arquivo removido: resumo_anual_2023.csv\n",
      "Baixado: resumo_anual_2024.csv\n",
      "Arquivo removido: resumo_anual_2024.csv\n",
      "Valores inválidos encontrados para Centro-Oeste:\n",
      "               date  latitude  longitude\n",
      "20240712 2024-07-12       -24        -61\n",
      "20240713 2024-07-13       -24        -61\n",
      "20240714 2024-07-14       -24        -61\n",
      "20240712 2024-07-12       -24        -57\n",
      "20240713 2024-07-13       -24        -57\n",
      "20240714 2024-07-14       -24        -57\n",
      "20240712 2024-07-12       -24        -53\n",
      "20240713 2024-07-13       -24        -53\n",
      "20240714 2024-07-14       -24        -53\n",
      "20240712 2024-07-12       -24        -49\n",
      "20240713 2024-07-13       -24        -49\n",
      "20240714 2024-07-14       -24        -49\n",
      "20240712 2024-07-12       -20        -61\n",
      "20240713 2024-07-13       -20        -61\n",
      "20240714 2024-07-14       -20        -61\n",
      "20240712 2024-07-12       -20        -57\n",
      "20240713 2024-07-13       -20        -57\n",
      "20240714 2024-07-14       -20        -57\n",
      "20240712 2024-07-12       -20        -53\n",
      "20240713 2024-07-13       -20        -53\n",
      "20240714 2024-07-14       -20        -53\n",
      "20240712 2024-07-12       -20        -49\n",
      "20240713 2024-07-13       -20        -49\n",
      "20240714 2024-07-14       -20        -49\n",
      "20240712 2024-07-12       -16        -61\n",
      "20240713 2024-07-13       -16        -61\n",
      "20240714 2024-07-14       -16        -61\n",
      "20240712 2024-07-12       -16        -57\n",
      "20240713 2024-07-13       -16        -57\n",
      "20240714 2024-07-14       -16        -57\n",
      "20240712 2024-07-12       -16        -53\n",
      "20240713 2024-07-13       -16        -53\n",
      "20240714 2024-07-14       -16        -53\n",
      "20240712 2024-07-12       -16        -49\n",
      "20240713 2024-07-13       -16        -49\n",
      "20240714 2024-07-14       -16        -49\n",
      "20240712 2024-07-12       -12        -61\n",
      "20240713 2024-07-13       -12        -61\n",
      "20240714 2024-07-14       -12        -61\n",
      "20240712 2024-07-12       -12        -57\n",
      "20240713 2024-07-13       -12        -57\n",
      "20240714 2024-07-14       -12        -57\n",
      "20240712 2024-07-12       -12        -53\n",
      "20240713 2024-07-13       -12        -53\n",
      "20240714 2024-07-14       -12        -53\n",
      "20240712 2024-07-12       -12        -49\n",
      "20240713 2024-07-13       -12        -49\n",
      "20240714 2024-07-14       -12        -49\n",
      "20240712 2024-07-12        -8        -61\n",
      "20240713 2024-07-13        -8        -61\n",
      "20240714 2024-07-14        -8        -61\n",
      "20240712 2024-07-12        -8        -57\n",
      "20240713 2024-07-13        -8        -57\n",
      "20240714 2024-07-14        -8        -57\n",
      "20240712 2024-07-12        -8        -53\n",
      "20240713 2024-07-13        -8        -53\n",
      "20240714 2024-07-14        -8        -53\n",
      "20240712 2024-07-12        -8        -49\n",
      "20240713 2024-07-13        -8        -49\n",
      "20240714 2024-07-14        -8        -49\n",
      "Dados agregados mensalmente para Centro-Oeste\n",
      "Processando dados para a região: Sudeste\n",
      "Valores inválidos encontrados para Sudeste:\n",
      "               date  latitude  longitude\n",
      "20240712 2024-07-12       -24        -48\n",
      "20240713 2024-07-13       -24        -48\n",
      "20240714 2024-07-14       -24        -48\n",
      "20240712 2024-07-12       -24        -44\n",
      "20240713 2024-07-13       -24        -44\n",
      "20240714 2024-07-14       -24        -44\n",
      "20240712 2024-07-12       -20        -48\n",
      "20240713 2024-07-13       -20        -48\n",
      "20240714 2024-07-14       -20        -48\n",
      "20240712 2024-07-12       -20        -44\n",
      "20240713 2024-07-13       -20        -44\n",
      "20240714 2024-07-14       -20        -44\n",
      "Dados agregados mensalmente para Sudeste\n",
      "Processando dados para a região: Sul\n",
      "Valores inválidos encontrados para Sul:\n",
      "               date  latitude  longitude\n",
      "20240712 2024-07-12       -34        -57\n",
      "20240713 2024-07-13       -34        -57\n",
      "20240714 2024-07-14       -34        -57\n",
      "20240712 2024-07-12       -34        -53\n",
      "20240713 2024-07-13       -34        -53\n",
      "20240714 2024-07-14       -34        -53\n",
      "20240712 2024-07-12       -34        -49\n",
      "20240713 2024-07-13       -34        -49\n",
      "20240714 2024-07-14       -34        -49\n",
      "20240712 2024-07-12       -30        -57\n",
      "20240713 2024-07-13       -30        -57\n",
      "20240714 2024-07-14       -30        -57\n",
      "20240712 2024-07-12       -30        -53\n",
      "20240713 2024-07-13       -30        -53\n",
      "20240714 2024-07-14       -30        -53\n",
      "20240712 2024-07-12       -30        -49\n",
      "20240713 2024-07-13       -30        -49\n",
      "20240714 2024-07-14       -30        -49\n",
      "20240712 2024-07-12       -26        -57\n",
      "20240713 2024-07-13       -26        -57\n",
      "20240714 2024-07-14       -26        -57\n",
      "20240712 2024-07-12       -26        -53\n",
      "20240713 2024-07-13       -26        -53\n",
      "20240714 2024-07-14       -26        -53\n",
      "20240712 2024-07-12       -26        -49\n",
      "20240713 2024-07-13       -26        -49\n",
      "20240714 2024-07-14       -26        -49\n",
      "Dados agregados mensalmente para Sul\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def get_all_data():\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submeter todas as funções get_data para execução paralela\n",
    "        future_to_data = {\n",
    "            executor.submit(get_bcb_data): 'bcb_data',\n",
    "            executor.submit(get_anfavea_production): 'anfavea_data',\n",
    "            executor.submit(get_sidra_data, sidra_list): 'sidra_data',\n",
    "            executor.submit(get_epe_data): 'epe_data',\n",
    "            executor.submit(get_market_data): 'market_data',\n",
    "            executor.submit(download_and_process_rpk_data): 'rpk_data',\n",
    "            executor.submit(get_climate_data): 'climate_data',\n",
    "            executor.submit(get_comex_data): 'comex_data',\n",
    "            executor.submit(get_unemployment_data): 'unemployment_data'\n",
    "        }\n",
    "\n",
    "        # Inicializar um dicionário para armazenar os resultados\n",
    "        data_frames = {}\n",
    "\n",
    "        # Coletar os resultados conforme as tarefas são concluídas\n",
    "        for future in concurrent.futures.as_completed(future_to_data):\n",
    "            data_name = future_to_data[future]\n",
    "            try:\n",
    "                data_frames[data_name] = future.result()\n",
    "            except Exception as exc:\n",
    "                print(f'{data_name} gerou uma exceção: {exc}')\n",
    "\n",
    "    return data_frames\n",
    "\n",
    "# Executar a função para obter todos os dados\n",
    "data_frames = get_all_data()\n",
    "\n",
    "# Fazer o join entre os DataFrames\n",
    "all_data = pd.concat([\n",
    "    data_frames['bcb_data'],\n",
    "    data_frames['anfavea_data'],\n",
    "    data_frames['sidra_data'],\n",
    "    data_frames['epe_data'],\n",
    "    data_frames['market_data'],\n",
    "    data_frames['rpk_data'],\n",
    "    data_frames['climate_data'],\n",
    "    data_frames['comex_data'],\n",
    "    data_frames['unemployment_data']\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o all data em um arquivo CSV\n",
    "all_data.to_csv(\"all_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
